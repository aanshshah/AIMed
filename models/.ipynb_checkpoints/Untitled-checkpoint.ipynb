{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supress warnings (especially from sklearn)\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "import random as rn\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "rn.seed(1254)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve, roc_curve, auc, precision_score, roc_curve, confusion_matrix, precision_recall_fscore_support, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.scorer import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.activations import softmax, relu\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import tensorflow\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> All Data Results </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    df = pd.read_csv('../data/x_lace_df.csv')\n",
    "    df = df.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    y = pd.read_csv('../data/y_more_no_df_clean.csv')\n",
    "    return df, y\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300,input_dim=64,activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(Dense(150,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[sensitivity])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, labels = preprocess()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.15)\n",
    "def run_pipeline():\n",
    "    epochs = 1000\n",
    "    batch_size = 3000\n",
    "    nn_grid = create_model()\n",
    "    num_classes = 2\n",
    "    nn_grid.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.around(nn_grid.predict(X_test))\n",
    "    return y_pred, nn_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "session = InteractiveSession(config=config)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred, nn_grid = run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> All Data Neural Network Results </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      4846\n",
      "           1       0.52      0.15      0.23       566\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5412\n",
      "   macro avg       0.71      0.57      0.59      5412\n",
      "weighted avg       0.87      0.90      0.87      5412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.models.save_model(nn_grid,\"./alldata_model.h5\", overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 0, n_jobs=-1)\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "K = 5\n",
    "skf = StratifiedKFold(n_splits=K)\n",
    "df, labels = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> All Data Baseline Results </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     32191\n",
      "           1       0.27      0.11      0.16      3885\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     36076\n",
      "   macro avg       0.59      0.54      0.55     36076\n",
      "weighted avg       0.83      0.87      0.85     36076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LR\n",
    "y_real = []\n",
    "y_predication = []\n",
    "for train_indices, test_indices in skf.split(df, labels):\n",
    "    Xtrain, Ytrain = df.iloc[train_indices], labels.iloc[train_indices]\n",
    "    Xvalid, Yvalid = df.iloc[test_indices], labels.iloc[test_indices]\n",
    "    y_real = y_real + Yvalid.values.tolist()\n",
    "    lr.fit(Xtrain, Ytrain)\n",
    "    y_preds = [0 if x < 0.5 else 1 for x in lr.predict(Xvalid)]\n",
    "    y_predication = y_predication + y_preds\n",
    "print(classification_report(y_real, y_predication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88     32191\n",
      "           1       0.13      0.17      0.14      3885\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     36076\n",
      "   macro avg       0.51      0.51      0.51     36076\n",
      "weighted avg       0.81      0.79      0.80     36076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "y_real = []\n",
    "y_predication = []\n",
    "for train_indices, test_indices in skf.split(df, labels):\n",
    "    Xtrain, Ytrain = df.iloc[train_indices], labels.iloc[train_indices]\n",
    "    Xvalid, Yvalid = df.iloc[test_indices], labels.iloc[test_indices]\n",
    "    y_real = y_real + Yvalid.values.tolist()\n",
    "    rf.fit(Xtrain, Ytrain)\n",
    "    y_preds = [0 if x < 0.5 else 1 for x in rf.predict(Xvalid)]\n",
    "    y_predication = y_predication + y_preds\n",
    "print(classification_report(y_real, y_predication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost(optimize=False):\n",
    "    x_df, y_df = preprocess()\n",
    "    xgb_opt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=0.6, gamma=0.0, learning_rate=0.1, max_delta_step=0, max_depth=2, min_child_weight=2, missing=None, n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic', random_state=0, reg_alpha=5.0, reg_lambda=1, scale_pos_weight=8.286036036036036, seed=1, silent=True, subsample=0.5)\n",
    "    K = 5\n",
    "    eval_size = int(np.round(1./K))\n",
    "    skf = StratifiedKFold(n_splits=K)\n",
    "    prediction = np.array([])\n",
    "    reals = np.array([])\n",
    "    name = 'XGBoost'\n",
    "    for train_indices, test_indices in skf.split(x_df, y_df):\n",
    "        X_train, y_train = x_df.iloc[train_indices], y_df.iloc[train_indices]\n",
    "        X_valid, y_valid = x_df.iloc[test_indices], y_df.iloc[test_indices]\n",
    "        class_weight_scale = 1.*y_df['label'].value_counts()[0]/y_df['label'].value_counts()[1]\n",
    "        xgb_opt.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "        xgb_opt.fit(X_train,y_train)\n",
    "        xgb_opt_pred_prob = xgb_opt.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        y_valid = y_valid.values.tolist()\n",
    "            \n",
    "        reals = np.append(reals,y_valid)\n",
    "        reals = reals.astype(int)\n",
    "        \n",
    "        prediction = np.append(prediction, xgb_opt_pred_prob)\n",
    "        prediction = prediction.astype(int)\n",
    "    print(classification_report(y_true = y_df.label, y_pred = xgb_opt.predict(x_df)))\n",
    "    print(classification_report(reals, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> All Data XGBoost Results </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.64      0.77     32191\n",
      "           1       0.20      0.75      0.32      3885\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     36076\n",
      "   macro avg       0.58      0.69      0.54     36076\n",
      "weighted avg       0.87      0.66      0.72     36076\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94     32191\n",
      "           1       0.00      0.00      0.00      3885\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     36076\n",
      "   macro avg       0.45      0.50      0.47     36076\n",
      "weighted avg       0.80      0.89      0.84     36076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/x_lace_df.csv')\n",
    "x_df = df.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "y_df = pd.read_csv('../data/y_more_no_df_clean.csv')\n",
    "class_weight_scale = 1.*y_df.label.value_counts()[0]/y_df.label.value_counts()[1]\n",
    "X_train = x_df\n",
    "y_train = y_df['label']\n",
    "xgb_opt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=0.6, gamma=0.0, learning_rate=0.1, max_delta_step=0, max_depth=2, min_child_weight=2, missing=None, n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic', random_state=0, reg_alpha=5.0, reg_lambda=1, scale_pos_weight=8.286036036036036, seed=1, silent=True, subsample=0.5)\n",
    "K = 5\n",
    "eval_size = int(np.round(1./K))\n",
    "skf = StratifiedKFold(n_splits=K)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "lw = 2\n",
    "i = 0\n",
    "roc_aucs_xgbopt = []\n",
    "for train_indices, test_indices in skf.split(x_df, y_df['label']):\n",
    "    X_train, y_train = x_df.iloc[train_indices], y_df['label'].iloc[train_indices]\n",
    "    X_valid, y_valid = x_df.iloc[test_indices], y_df['label'].iloc[test_indices]\n",
    "    class_weight_scale = 1.*y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "    xgb_opt.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "    xgb_opt.fit(X_train,y_train)\n",
    "    xgb_opt_pred_prob = xgb_opt.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(classification_report(y_true = y_df.label, y_pred = xgb_opt.predict(x_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST Results\n",
    "\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.6, gamma=0.0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=2, min_child_weight=2, missing=None,\n",
    "       n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=5.0, reg_lambda=1,\n",
    "       scale_pos_weight=8.286036036036036, seed=1, silent=True,\n",
    "       subsample=0.5)\n",
    "       \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.64      0.77     32191\n",
    "           1       0.20      0.75      0.32      3885\n",
    "\n",
    "       micro avg       0.66      0.66      0.66     36076\n",
    "       macro avg       0.58      0.69      0.54     36076\n",
    "       weighted avg    0.87      0.66      0.72     36076"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Clustered Data Results </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    data = pd.read_csv('../data/labeled_clustered_data.csv')\n",
    "    df_0 = data[(data[['cluster_num']] == 0).any(axis=1)]\n",
    "    df_0_label=df_0.pop('label')\n",
    "    df_1 = data[(data[['cluster_num']] == 1).any(axis=1)]\n",
    "    df_1_label=df_1.pop('label')\n",
    "    df_2 = data[(data[['cluster_num']] == 2).any(axis=1)]\n",
    "    df_2_label=df_2.pop('label')\n",
    "    dfs = [df_0, df_1, df_2]\n",
    "    dfs_labels = [df_0_label, df_1_label, df_2_label]\n",
    "    return dfs, dfs_labels\n",
    "\n",
    "def create_pipeline_baseline():\n",
    "    pipeline = []\n",
    "    skf = StratifiedKFold(n_splits=2)\n",
    "    lr = GridSearchCV(LogisticRegression(random_state = 0), cv=skf, verbose=0, param_grid={})\n",
    "    rf = GridSearchCV(RandomForestClassifier(random_state=0), cv=skf, verbose=0, param_grid={})\n",
    "    pipeline = [['LogisticRegression', lr], ['RandomForest',rf]]\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    dfs, dfs_labels = preprocess()\n",
    "    print('finished preprocess')\n",
    "    baseline_grid = create_pipeline_baseline()\n",
    "    print('created pipeline and running ...')\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    for i, df in enumerate(dfs):\n",
    "        labels = dfs_labels[i]\n",
    "        opt_df, _, labels, _ = train_test_split(df, labels, test_size=0)\n",
    "        for name, grid in baseline_grid:\n",
    "            prediction = np.array([])\n",
    "            reals = np.array([])\n",
    "            for train_indices, test_indices in skf.split(opt_df, labels):\n",
    "                X_train, y_train = opt_df.iloc[train_indices], labels.iloc[train_indices]\n",
    "                X_valid, y_valid = opt_df.iloc[test_indices], labels.iloc[test_indices]\n",
    "                grid.fit(X_train, y_train)\n",
    "                y_pred = np.around(grid.predict(X_valid))\n",
    "                reals = np.append(reals,y_valid)\n",
    "                reals = reals.astype(int)\n",
    "                prediction = np.append(prediction, y_pred)\n",
    "                prediction = prediction.astype(int)\n",
    "            print(name, 'cluster: ' + str(i))\n",
    "            print(classification_report(reals, prediction))\n",
    "        print('RUN_XGBOOST CLUSTER: '+str(i))\n",
    "        if i == 0:\n",
    "            xgb_opt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.4, gamma=0.0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=1, min_child_weight=1, missing=None,\n",
    "       n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=1.0000000000000002e-06, reg_lambda=1,\n",
    "       scale_pos_weight=10.186375321336762, seed=1, silent=True,\n",
    "        subsample=0.2)\n",
    "        elif i == 1:\n",
    "            xgb_opt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5, gamma=0.0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=2, min_child_weight=3, missing=None,\n",
    "       n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=8.653391412570006, seed=1, silent=True,\n",
    "       subsample=0.6)\n",
    "        else:\n",
    "            xgb_opt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5, gamma=0.1, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=5, min_child_weight=6, missing=None,\n",
    "       n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=0.0005, reg_lambda=1,\n",
    "       scale_pos_weight=5.418508287292818, seed=1, silent=True,\n",
    "       subsample=0.7)\n",
    "        skf = StratifiedKFold(n_splits=5)        \n",
    "        name = 'XGBoost'\n",
    "        fold = 0\n",
    "        prediction = np.array([])\n",
    "        reals = np.array([])\n",
    "        for train_indices, test_indices in skf.split(opt_df, labels):\n",
    "            X_train, y_train = opt_df.iloc[train_indices], labels.iloc[train_indices]\n",
    "            X_valid, y_valid = opt_df.iloc[test_indices], labels.iloc[test_indices]\n",
    "            class_weight_scale = 1.*y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "            xgb_opt.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "            xgb_opt.fit(X_train,y_train)\n",
    "            xgb_opt_pred_prob = xgb_opt.predict_proba(X_valid)[:, 1]\n",
    "            xgb_opt_pred_prob = np.around(xgb_opt_pred_prob)\n",
    "            y_valid = y_valid.values.tolist()\n",
    "            reals = np.append(reals,y_valid)\n",
    "            reals = reals.astype(int)\n",
    "            prediction = np.append(prediction, xgb_opt_pred_prob)\n",
    "            prediction = prediction.astype(int)\n",
    "        print(classification_report(y_true = labels, y_pred = xgb_opt.predict(opt_df)))\n",
    "        print(classification_report(reals, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocess\n",
      "created pipeline and running ...\n",
      "LogisticRegression cluster: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95      9906\n",
      "           1       0.40      0.05      0.10       972\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     10878\n",
      "   macro avg       0.65      0.52      0.52     10878\n",
      "weighted avg       0.87      0.91      0.88     10878\n",
      "\n",
      "RandomForest cluster: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      9906\n",
      "           1       0.47      0.04      0.07       972\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     10878\n",
      "   macro avg       0.69      0.52      0.51     10878\n",
      "weighted avg       0.87      0.91      0.87     10878\n",
      "\n",
      "RUN_XGBOOST CLUSTER: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82      9906\n",
      "           1       0.19      0.67      0.30       972\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     10878\n",
      "   macro avg       0.57      0.70      0.56     10878\n",
      "weighted avg       0.89      0.71      0.77     10878\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82      9906\n",
      "           1       0.18      0.66      0.29       972\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     10878\n",
      "   macro avg       0.57      0.69      0.55     10878\n",
      "weighted avg       0.89      0.71      0.77     10878\n",
      "\n",
      "LogisticRegression cluster: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94     17382\n",
      "           1       0.46      0.06      0.10      2008\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     19390\n",
      "   macro avg       0.68      0.53      0.52     19390\n",
      "weighted avg       0.86      0.90      0.86     19390\n",
      "\n",
      "RandomForest cluster: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     17382\n",
      "           1       0.53      0.04      0.08      2008\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     19390\n",
      "   macro avg       0.71      0.52      0.51     19390\n",
      "weighted avg       0.86      0.90      0.86     19390\n",
      "\n",
      "RUN_XGBOOST CLUSTER: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80     17382\n",
      "           1       0.21      0.72      0.33      2008\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     19390\n",
      "   macro avg       0.58      0.71      0.57     19390\n",
      "weighted avg       0.88      0.70      0.75     19390\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.70      0.80     17382\n",
      "           1       0.21      0.68      0.32      2008\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     19390\n",
      "   macro avg       0.58      0.69      0.56     19390\n",
      "weighted avg       0.87      0.70      0.75     19390\n",
      "\n",
      "LogisticRegression cluster: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.92      4903\n",
      "           1       0.53      0.08      0.13       905\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      5808\n",
      "   macro avg       0.69      0.53      0.52      5808\n",
      "weighted avg       0.80      0.85      0.79      5808\n",
      "\n",
      "RandomForest cluster: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      4903\n",
      "           1       0.39      0.06      0.10       905\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      5808\n",
      "   macro avg       0.62      0.52      0.51      5808\n",
      "weighted avg       0.78      0.84      0.78      5808\n",
      "\n",
      "RUN_XGBOOST CLUSTER: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91      4903\n",
      "           1       0.52      0.83      0.64       905\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      5808\n",
      "   macro avg       0.74      0.85      0.78      5808\n",
      "weighted avg       0.90      0.86      0.87      5808\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.86      4903\n",
      "           1       0.36      0.53      0.43       905\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      5808\n",
      "   macro avg       0.63      0.68      0.65      5808\n",
      "weighted avg       0.82      0.78      0.80      5808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    data = pd.read_csv('../data/labeled_clustered_data.csv')\n",
    "    df_0 = data[(data[['cluster_num']] == 0).any(axis=1)]\n",
    "    df_0_label=df_0.pop('label')\n",
    "    df_1 = data[(data[['cluster_num']] == 1).any(axis=1)]\n",
    "    df_1_label=df_1.pop('label')\n",
    "    df_2 = data[(data[['cluster_num']] == 2).any(axis=1)]\n",
    "    df_2_label=df_2.pop('label')\n",
    "    dfs = [df_0, df_1, df_2]\n",
    "    dfs_labels = [df_0_label, df_1_label, df_2_label]\n",
    "    return dfs, dfs_labels\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "df, labels = preprocess()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_0_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5,input_dim=66,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[sensitivity])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_1_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5,input_dim=66,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[sensitivity])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_2_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5,input_dim=66,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[sensitivity])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    dfs, dfs_labels = preprocess()\n",
    "    print('finished preprocess')\n",
    "    print('created pipeline and running ...')\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    epochs = 500\n",
    "    batch_size = 1028\n",
    "    num_classes = 2\n",
    "    df0 = df[0]\n",
    "    labels0 = dfs_labels[0]\n",
    "    df1 = df[1]\n",
    "    labels1 = dfs_labels[1]\n",
    "    df2 = df[2]\n",
    "    labels2 = dfs_labels[2]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df0, labels0, test_size=.2, random_state=2)\n",
    "    nn_grid = create_cluster_0_model()\n",
    "    nn_grid.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    y_pred = np.around(nn_grid.predict(X_test))\n",
    "    print('CLUSTER 0:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df1, labels1, test_size=.2, random_state=2)\n",
    "    nn_grid = create_cluster_1_model()\n",
    "    nn_grid.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    y_pred = np.around(nn_grid.predict(X_test))\n",
    "    print('CLUSTER 1:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "#     epochs = 100\n",
    "#     batch_size = 124\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df2, labels2, test_size=.2, random_state=2)\n",
    "    nn_grid = create_cluster_2_model()\n",
    "    nn_grid.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    y_pred = np.around(nn_grid.predict(X_test))\n",
    "    print('CLUSTER 2:')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocess\n",
      "created pipeline and running ...\n",
      "CLUSTER 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95      1967\n",
      "           1       0.00      0.00      0.00       209\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2176\n",
      "   macro avg       0.45      0.50      0.47      2176\n",
      "weighted avg       0.82      0.90      0.86      2176\n",
      "\n",
      "CLUSTER 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95      3486\n",
      "           1       0.39      0.03      0.05       392\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      3878\n",
      "   macro avg       0.65      0.51      0.50      3878\n",
      "weighted avg       0.85      0.90      0.86      3878\n",
      "\n",
      "CLUSTER 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.92       997\n",
      "           1       0.61      0.07      0.12       165\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1162\n",
      "   macro avg       0.74      0.53      0.52      1162\n",
      "weighted avg       0.83      0.86      0.81      1162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocess\n",
      "created pipeline and running ...\n",
      "CLUSTER 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95      1967\n",
      "           1       0.00      0.00      0.00       209\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2176\n",
      "   macro avg       0.45      0.50      0.47      2176\n",
      "weighted avg       0.82      0.90      0.86      2176\n",
      "\n",
      "CLUSTER 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      3486\n",
      "           1       0.37      0.19      0.25       392\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      3878\n",
      "   macro avg       0.64      0.58      0.59      3878\n",
      "weighted avg       0.86      0.88      0.87      3878\n",
      "\n",
      "CLUSTER 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       997\n",
      "           1       0.00      0.00      0.00       165\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1162\n",
      "   macro avg       0.43      0.50      0.46      1162\n",
      "weighted avg       0.74      0.86      0.79      1162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs, dfs_labels = preprocess()\n",
    "# for df in dfs:\n",
    "#     print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
