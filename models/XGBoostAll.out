performing hyperparamter optimization step 0
{'n_estimators': 50} 0.7407405280840648
performing hyperparamter optimization step 1
{'max_depth': 2, 'min_child_weight': 2} 0.7557653033792218
performing hyperparamter optimization step 2
{'gamma': 0.0} 0.7557653033792218
performing hyperparamter optimization step 3
{'colsample_bytree': 0.6, 'subsample': 0.5} 0.7625142019993799
performing hyperparamter optimization step 4
{'reg_alpha': 10} 0.7626329087866113
performing hyperparamter optimization step 4b
{'reg_alpha': 5.0} 0.7626329087866113

Parameter optimization finished!
OPTIMAL FOR ALL DATA:
XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=0.6, gamma=0.0, learning_rate=0.1,
       max_delta_step=0, max_depth=2, min_child_weight=2, missing=None,
       n_estimators=50, n_jobs=1, nthread=4, objective='binary:logistic',
       random_state=0, reg_alpha=5.0, reg_lambda=1,
       scale_pos_weight=8.285971685971687, seed=1, silent=True,
       subsample=0.5)
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
              precision    recall  f1-score   support

           0       0.89      1.00      0.94     32191
           1       0.00      0.00      0.00      3885

   micro avg       0.89      0.89      0.89     36076
   macro avg       0.45      0.50      0.47     36076
weighted avg       0.80      0.89      0.84     36076

