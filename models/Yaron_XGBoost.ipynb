{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import all relevant libraries\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/x_with_lacefeatures.csv')\n",
    "x_df = df.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "y_df = pd.read_csv('../data/y_more_no_df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.285971685971687"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the class weight scale (a hyperparameter) as the ration of negative labels to positive labels.\n",
    "# This instructs the classifier to address the class imbalance.\n",
    "class_weight_scale = 1.*y_df.label.value_counts()[0]/y_df.label.value_counts()[1]\n",
    "class_weight_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=8.285971685971687,\n",
       "       seed=1, silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting minimal required initial hyperparameters\n",
    "\n",
    "param={\n",
    "    'objective':'binary:logistic',\n",
    "    'nthread':4,\n",
    "    'scale_pos_weight':class_weight_scale,\n",
    "    'seed' : 1   \n",
    "}\n",
    "xgb1 = XGBClassifier()\n",
    "xgb1.set_params(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cabb668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train initial classifier and analyze performace using K-fold cross-validation \n",
    "K = 5\n",
    "eval_size = int(np.round(1./K))\n",
    "skf = StratifiedKFold(n_splits=K)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "lw = 2\n",
    "i = 0\n",
    "roc_aucs_xgb1 = []\n",
    "for train_indices, test_indices in skf.split(x_df, y_df['label']):\n",
    "    X_train, y_train = x_df.iloc[train_indices], y_df['label'].iloc[train_indices]\n",
    "    X_valid, y_valid = x_df.iloc[test_indices], y_df['label'].iloc[test_indices]\n",
    "    class_weight_scale = 1.*y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "    xgb1.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "    xgb1.fit(X_train,y_train)\n",
    "    xgb1_pred_prob = xgb1.predict_proba(X_valid)\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, xgb1_pred_prob[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_aucs_xgb1.append(roc_auc)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "mean_tpr /= K\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing hyperparamter optimization step 0\n",
      "{'n_estimators': 210} 0.8328052753455925\n",
      "performing hyperparamter optimization step 1\n"
     ]
    }
   ],
   "source": [
    "X_train = x_df\n",
    "y_train = y_df['label']\n",
    "\n",
    "if optimize:\n",
    "   \n",
    "    param_test0 = {\n",
    "     'n_estimators':range(50,250,10)\n",
    "    }\n",
    "    print ('performing hyperparamter optimization step 0')\n",
    "    gsearch0 = GridSearchCV(estimator = xgb1, param_grid = param_test0, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "    gsearch0.fit(X_train,y_train)\n",
    "    print (gsearch0.best_params_, gsearch0.best_score_)\n",
    "\n",
    "    param_test1 = {\n",
    "     'max_depth':range(1,10),\n",
    "     'min_child_weight':range(1,10)\n",
    "    }\n",
    "    print ('performing hyperparamter optimization step 1')\n",
    "    gsearch1 = GridSearchCV(estimator = gsearch0.best_estimator_,\n",
    "     param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "    gsearch1.fit(X_train,y_train)\n",
    "    print (gsearch1.best_params_, gsearch1.best_score_)\n",
    "\n",
    "    max_d = gsearch1.best_params_['max_depth']\n",
    "    min_c = gsearch1.best_params_['min_child_weight']\n",
    "    \n",
    "    param_test2 = {\n",
    "     'gamma':[i/10. for i in range(0,5)]\n",
    "    }\n",
    "    print ('performing hyperparamter optimization step 2')\n",
    "    gsearch2 = GridSearchCV(estimator = gsearch1.best_estimator_, \n",
    "     param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "    gsearch2.fit(X_train,y_train)\n",
    "    print (gsearch2.best_params_, gsearch2.best_score_)\n",
    "\n",
    "    param_test3 = {\n",
    "        'subsample':[i/10.0 for i in range(1,10)],\n",
    "        'colsample_bytree':[i/10.0 for i in range(1,10)]\n",
    "    }\n",
    "    print ('performing hyperparamter optimization step 3')\n",
    "    gsearch3 = GridSearchCV(estimator = gsearch2.best_estimator_, \n",
    "     param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "    gsearch3.fit(X_train,y_train)\n",
    "    print (gsearch3.best_params_, gsearch3.best_score_)\n",
    "\n",
    "    param_test4 = {\n",
    "        'reg_alpha':[0, 1e-5, 1e-3, 0.1, 10]\n",
    "    }\n",
    "    print ('performing hyperparamter optimization step 4')\n",
    "    gsearch4 = GridSearchCV(estimator = gsearch3.best_estimator_, \n",
    "     param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "    gsearch4.fit(X_train,y_train)\n",
    "    print (gsearch4.best_params_, gsearch4.best_score_)\n",
    "\n",
    "    alpha = gsearch4.best_params_['reg_alpha']\n",
    "    if alpha != 0:\n",
    "        param_test4b = {\n",
    "            'reg_alpha':[0.1*alpha, 0.25*alpha, 0.5*alpha, alpha, 2.5*alpha, 5*alpha, 10*alpha]\n",
    "        }\n",
    "        print ('performing hyperparamter optimization step 4b')\n",
    "        gsearch4b = GridSearchCV(estimator = gsearch4.best_estimator_, \n",
    "         param_grid = param_test4b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "        gsearch4b.fit(X_train,y_train)\n",
    "        print (gsearch4b.best_params_, gsearch4.best_score_)\n",
    "        print ('\\nParameter optimization finished!')\n",
    "        xgb_opt = gsearch4b.best_estimator_\n",
    "        xgb_opt\n",
    "    else:\n",
    "        xgb_opt = gsearch4.best_estimator_\n",
    "        xgb_opt\n",
    "else: \n",
    "    # Pre-optimized settings\n",
    "    xgb_opt = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "       gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=5, missing=None, n_estimators=70, nthread=4,\n",
    "       objective='binary:logistic', reg_alpha=25.0, reg_lambda=1,\n",
    "       scale_pos_weight=7.0909090909090908, seed=1, silent=True,\n",
    "       subsample=0.6)\n",
    "    \n",
    "print (xgb_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "eval_size = int(np.round(1./K))\n",
    "skf = StratifiedKFold(n_splits=K)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "lw = 2\n",
    "i = 0\n",
    "roc_aucs_xgbopt = []\n",
    "for train_indices, test_indices in skf.split(x_df, y_df['label']):\n",
    "    X_train, y_train = x_df.iloc[train_indices], y_df['label'].iloc[train_indices]\n",
    "    X_valid, y_valid = x_df.iloc[test_indices], y_df['label'].iloc[test_indices]\n",
    "    class_weight_scale = 1.*y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "    xgb_opt.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "    xgb_opt.fit(X_train,y_train)\n",
    "    xgb_opt_pred_prob = xgb_opt.predict_proba(X_valid)\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, xgb_opt_pred_prob[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_aucs_xgbopt.append(roc_auc)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "mean_tpr /= K\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true = y_df.label, y_pred = xgb_opt.predict(x_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_plot_importance(booster, figsize, **kwargs): \n",
    "    from matplotlib import pyplot as plt\n",
    "    from xgboost import plot_importance\n",
    "    fig, ax = plt.subplots(1,1,figsize=(figsize))\n",
    "    plot_importance(booster=booster, ax=ax, **kwargs)\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label,] +\n",
    "ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(10)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('figures/Feature_importance.png')\n",
    "\n",
    "my_plot_importance(xgb_opt, (5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
