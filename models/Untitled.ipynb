{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supress warnings (especially from sklearn)\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import csv\n",
    "from scipy import interp\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve, roc_curve, auc, precision_score, roc_curve, confusion_matrix, precision_recall_fscore_support, f1_score, precision_score, recall_score, accuracy_score\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.activations import softmax, relu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import tensorflow\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "def classification_report_csv(y_test, y_preds, name, full_path, new_file=True):\n",
    "    report = classification_report(y_test, y_preds)\n",
    "    report = [x.strip().split() for x in report.strip().split('\\n') if x]\n",
    "    report[0] = ['model','class'] + report[0]\n",
    "    report[3] = [''.join(report[3][0:3])] + report[3][3:]\n",
    "    data = np.array([[name] + report[i] for i in range(1,len(report))])\n",
    "    report_df = pd.DataFrame(data, columns=np.array(report)[0])\n",
    "    if new_file: report_df.to_csv(full_path, index = False)\n",
    "    else: report_df.to_csv(open(full_path, 'a'), index=False, header=False)\n",
    "\n",
    "def store_cluster_info(y_preds, y_real, name, new_file=True):\n",
    "    filename = 'results/neuralnet_alldata_results.csv'\n",
    "    classification_report_csv(y_real, y_preds, name, filename, new_file)\n",
    "\n",
    "def preprocess():\n",
    "    df = pd.read_csv('../data/x_with_lacefeatures.csv')\n",
    "    df = df.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    y = pd.read_csv('../data/y_more_no_df_clean.csv')\n",
    "    return df, y\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "def create_model(first_neuron=64, second_neuron=32, second_activation='relu', \n",
    "                 last_neuron=1, last_activation='relu', loss='binary_crossentropy', \n",
    "                 optimizer='adam', lr=0.01, dropout=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_neuron,input_dim=68,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(second_neuron,activation=second_activation))\n",
    "    model.add(Dense(last_neuron,activation=last_activation))\n",
    "    model.compile(loss=loss, optimizer=optimizer(lr=lr), metrics=[sensitivity])\n",
    "    return model\n",
    "\n",
    "def create_pipeline_baseline():\n",
    "    pipeline = []\n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "    lr = GridSearchCV(LogisticRegression(random_state = 0), cv=skf, verbose=0, param_grid={})\n",
    "    rf = GridSearchCV(RandomForestClassifier(random_state=0), cv=skf, verbose=0, param_grid={})\n",
    "    pipeline = [['LogisticRegression', lr], ['RandomForest',rf]]\n",
    "    return pipeline\n",
    "\n",
    "def create_pipeline_nn():\n",
    "    param_grid = {'clf__lr': [0.01],\n",
    "     'clf__first_neuron':[8, 16, 32, 64, 128, 256],\n",
    "     'clf__second_neuron':[1, 8, 32, 64, 128],\n",
    "    'clf__last_neuron':[1],\n",
    "     'clf__batch_size': [64, 128],\n",
    "     'clf__epochs': [20],\n",
    "     'clf__dropout': [0, 0.2],\n",
    "     'clf__optimizer': [Adam],\n",
    "     'clf__loss': [binary_crossentropy],\n",
    "          'clf__second_activation': [relu],\n",
    "     'clf__last_activation': [relu]}\n",
    "    clf = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "    scaler = StandardScaler()\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess',scaler),\n",
    "        ('clf',clf)\n",
    "    ])\n",
    "    scorers = {\n",
    "    'precision_score': make_scorer(precision_score)\n",
    "    # 'recall_score': make_scorer(recall_score),\n",
    "    # 'accuracy_score': make_scorer(accuracy_score)\n",
    "    }\n",
    "    refit_score='precision_score'\n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "    grid = GridSearchCV(pipeline, cv=skf, param_grid=param_grid, verbose=3, scoring=scorers,refit=refit_score)\n",
    "    return grid\n",
    "\n",
    "def run_pipeline():\n",
    "    df, labels = preprocess()\n",
    "    print('finished preprocessing')\n",
    "    opt_df, _, labels, _ = train_test_split(df, labels, test_size=0)\n",
    "    nn_grid = create_pipeline_nn()\n",
    "\n",
    "    print('created pipeline and running ...')\n",
    "\n",
    "    nn_grid.fit(opt_df, labels)\n",
    "\n",
    "\n",
    "    nn_opt = nn_grid.best_estimator_\n",
    "    print(nn_opt)\n",
    "    pd.DataFrame(nn_grid.cv_results_).to_csv('neuralnet_optimal_all_data.csv')\n",
    "    print('fitting based on optimal parameters')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.15)\n",
    "    nn_opt.fit(X_train, y_train)\n",
    "    y_pred = nn_opt.predict(X_test)\n",
    "    name = 'Neural_Network'\n",
    "    store_cluster_info(y_pred, y_test, name, new_file=True)\n",
    "\n",
    "\n",
    "\n",
    "config = ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "session = InteractiveSession(config=config)\n",
    "set_session(session)\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing hyperparamter optimization step 0\n"
     ]
    }
   ],
   "source": [
    "#supress warnings (especially from sklearn)\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve, roc_curve, auc, precision_score, roc_curve, confusion_matrix, precision_recall_fscore_support, f1_score, precision_score, recall_score, accuracy_score\n",
    "import csv\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from scipy import interp\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "# import joblib\n",
    "\n",
    "def classification_report_csv(y_test, y_preds, name, full_path, new_file=True):\n",
    "    report = classification_report(y_test, y_preds)\n",
    "    report = [x.strip().split() for x in report.strip().split('\\n') if x]\n",
    "    report[0] = ['model','class'] + report[0]\n",
    "    report[3] = [''.join(report[3][0:3])] + report[3][3:]\n",
    "    data = np.array([[name] + report[i] for i in range(1,len(report))])\n",
    "    report_df = pd.DataFrame(data, columns=np.array(report)[0])\n",
    "    if new_file: report_df.to_csv(full_path, index = False)\n",
    "    else: report_df.to_csv(open(full_path, 'a'), index=False, header=False)\n",
    "\n",
    "def store_cluster_info(y_preds, y_real, name, new_file=True):\n",
    "    filename = 'results/alldata_results_xgboost.csv'\n",
    "    classification_report_csv(y_real, y_preds, name, filename, new_file)\n",
    "\n",
    "def preprocess():\n",
    "    df = pd.read_csv('../data/x_with_lacefeatures.csv')\n",
    "    df = df.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    y = pd.read_csv('../data/y_more_no_df_clean.csv')\n",
    "    return df, y\n",
    "\n",
    "def run_xgboost(optimize=True):\n",
    "    x_df, y_df = preprocess()\n",
    "    if optimize:\n",
    "        xgb_opt = opt_xgboost(x_df, y_df, optimize)\n",
    "    else:\n",
    "        xgb_opt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "        colsample_bytree=0.9, gamma=0.0, learning_rate=0.1,\n",
    "        max_delta_step=0, max_depth=3, min_child_weight=7, missing=None,\n",
    "        n_estimators=210, n_jobs=1, nthread=4, objective='binary:logistic',\n",
    "        random_state=0, reg_alpha=25.0, reg_lambda=1,\n",
    "        scale_pos_weight=8.285971685971687, seed=1, silent=True,\n",
    "        subsample=0.9)\n",
    "    K = 5\n",
    "    eval_size = int(np.round(1./K))\n",
    "    skf = StratifiedKFold(n_splits=K)\n",
    "\n",
    "    prediction = np.array([])\n",
    "    reals = np.array([])\n",
    "    name = 'XGBoost'\n",
    "    for train_indices, test_indices in skf.split(x_df, y_df):\n",
    "        X_train, y_train = x_df.iloc[train_indices], y_df.iloc[train_indices]\n",
    "        X_valid, y_valid = x_df.iloc[test_indices], y_df.iloc[test_indices]\n",
    "        class_weight_scale = 1.*y_df['label'].value_counts()[0]/y_df['label'].value_counts()[1]\n",
    "        print('class weight scale : {}'.format(class_weight_scale))\n",
    "        xgb_opt.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "        xgb_opt.fit(X_train,y_train)\n",
    "        xgb_opt_pred_prob = xgb_opt.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        y_valid = y_valid.values.tolist()\n",
    "            \n",
    "        reals = np.append(reals,y_valid)\n",
    "        reals = reals.astype(int)\n",
    "        \n",
    "        prediction = np.append(prediction, xgb_opt_pred_prob)\n",
    "        prediction = prediction.astype(int)\n",
    "    store_cluster_info(prediction, reals, name, new_file=False)\n",
    "    # store_cluster_info(y_predications, y_real, name, cluster) \n",
    "\n",
    "def opt_xgboost(x_df, y_df, optimize=True):\n",
    "    # Define the class weight scale (a hyperparameter) as the ration of negative labels to positive labels.\n",
    "    # This instructs the classifier to address the class imbalance.\n",
    "    class_weight_scale = 1.*y_df['label'].value_counts()[0]/y_df['label'].value_counts()[1]\n",
    "    filepath = 'results/figures/'\n",
    "    # Setting minimal required initial hyperparameters\n",
    "    param={\n",
    "        'objective':'binary:logistic',\n",
    "        'nthread':4,\n",
    "        'scale_pos_weight':class_weight_scale,\n",
    "        'seed' : 1   \n",
    "    }\n",
    "    xgb1 = XGBClassifier()\n",
    "    xgb1.set_params(**param)\n",
    "    K = 5\n",
    "    eval_size = int(np.round(1./K))\n",
    "    skf = StratifiedKFold(n_splits=K)\n",
    "\n",
    "    for train_indices, test_indices in skf.split(x_df, y_df):\n",
    "        X_train, y_train = x_df.iloc[train_indices], y_df.iloc[train_indices]\n",
    "        X_valid, y_valid = x_df.iloc[test_indices], y_df.iloc[test_indices]\n",
    "        class_weight_scale = 1.*y_df['label'].value_counts()[0]/y_df['label'].value_counts()[1]\n",
    "        xgb1.set_params(**{'scale_pos_weight' : class_weight_scale})\n",
    "        xgb1.fit(X_train,y_train)\n",
    "\n",
    "    X_train = x_df\n",
    "    y_train = y_df\n",
    "\n",
    "    if optimize:\n",
    "        scorers = {\n",
    "        'precision_score': make_scorer(precision_score)\n",
    "        }\n",
    "\n",
    "        param_test0 = {\n",
    "         'n_estimators':range(50,250,10)\n",
    "        }\n",
    "        print('performing hyperparamter optimization step 0')\n",
    "        gsearch0 = GridSearchCV(estimator = xgb1, param_grid = param_test0, n_jobs=-1, scoring=scorers,iid=False, cv=5, refit='precision_score')\n",
    "        gsearch0.fit(X_train,y_train)\n",
    "        print(gsearch0.best_params_, gsearch0.best_score_)\n",
    "\n",
    "        param_test1 = {\n",
    "         'max_depth':range(1,10),\n",
    "         'min_child_weight':range(1,10)\n",
    "        }\n",
    "        print('performing hyperparamter optimization step 1')\n",
    "        gsearch1 = GridSearchCV(estimator = gsearch0.best_estimator_,\n",
    "         param_grid = param_test1, scoring=scorers, n_jobs=-1, iid=False, cv=5, refit='precision_score')\n",
    "        gsearch1.fit(X_train,y_train)\n",
    "        print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "\n",
    "        max_d = gsearch1.best_params_['max_depth']\n",
    "        min_c = gsearch1.best_params_['min_child_weight']\n",
    "\n",
    "        param_test2 = {\n",
    "         'gamma':[i/10. for i in range(0,5)]\n",
    "        }\n",
    "        print('performing hyperparamter optimization step 2')\n",
    "        gsearch2 = GridSearchCV(estimator = gsearch1.best_estimator_, \n",
    "         param_grid = param_test2, scoring=scorers, n_jobs=-1, iid=False, cv=5, refit='precision_score')\n",
    "        gsearch2.fit(X_train,y_train)\n",
    "        print(gsearch2.best_params_, gsearch2.best_score_)\n",
    "\n",
    "        param_test3 = {\n",
    "            'subsample':[i/10.0 for i in range(1,10)],\n",
    "            'colsample_bytree':[i/10.0 for i in range(1,10)]\n",
    "        }\n",
    "        print('performing hyperparamter optimization step 3')\n",
    "        gsearch3 = GridSearchCV(estimator = gsearch2.best_estimator_, \n",
    "         param_grid = param_test3, scoring=scorers, n_jobs=-1, iid=False, cv=5, refit='precision_score')\n",
    "        gsearch3.fit(X_train,y_train)\n",
    "        print(gsearch3.best_params_, gsearch3.best_score_)\n",
    "\n",
    "        param_test4 = {\n",
    "            'reg_alpha':[0, 1e-5, 1e-3, 0.1, 10]\n",
    "        }\n",
    "        print('performing hyperparamter optimization step 4')\n",
    "        gsearch4 = GridSearchCV(estimator = gsearch3.best_estimator_, \n",
    "         param_grid = param_test4, scoring=scorers, n_jobs=-1, iid=False, cv=5, refit='precision_score')\n",
    "        gsearch4.fit(X_train,y_train)\n",
    "        print(gsearch4.best_params_, gsearch4.best_score_)\n",
    "\n",
    "        alpha = gsearch4.best_params_['reg_alpha']\n",
    "        if alpha != 0:\n",
    "            param_test4b = {\n",
    "                'reg_alpha':[0.1*alpha, 0.25*alpha, 0.5*alpha, alpha, 2.5*alpha, 5*alpha, 10*alpha]\n",
    "            }\n",
    "            print('performing hyperparamter optimization step 4b')\n",
    "            gsearch4b = GridSearchCV(estimator = gsearch4.best_estimator_, \n",
    "             param_grid = param_test4b, scoring=scorers,n_jobs=-1,iid=False, cv=5, refit='precision_score')\n",
    "            gsearch4b.fit(X_train,y_train)\n",
    "            print(gsearch4b.best_params_, gsearch4.best_score_)\n",
    "            print('\\nParameter optimization finished!')\n",
    "            xgb_opt = gsearch4b.best_estimator_\n",
    "        else:\n",
    "            xgb_opt = gsearch4.best_estimator_\n",
    "    else: \n",
    "        # Pre-optimized settings\n",
    "        xgb_opt = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "           gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "           min_child_weight=5, missing=None, n_estimators=70, nthread=4,\n",
    "           objective='binary:logistic', reg_alpha=25.0, reg_lambda=1,\n",
    "           scale_pos_weight=7.0909090909090908, seed=1, silent=True,\n",
    "           subsample=0.6)\n",
    "    print('OPTIMAL FOR ALL DATA:')\n",
    "    print(xgb_opt)\n",
    "    #save model\n",
    "    return xgb_opt\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_xgboost(optimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
