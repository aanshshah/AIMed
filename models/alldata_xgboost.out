performing hyperparamter optimization step 0
{'n_estimators': 210} 0.832805275348697
performing hyperparamter optimization step 1
{'max_depth': 3, 'min_child_weight': 7} 0.8333741400745287
performing hyperparamter optimization step 2
{'gamma': 0.0} 0.8333741400745287
performing hyperparamter optimization step 3
{'colsample_bytree': 0.9, 'subsample': 0.8} 0.8335927965757449
performing hyperparamter optimization step 4
{'reg_alpha': 10} 0.8361123368260459
performing hyperparamter optimization step 4b
{'reg_alpha': 25.0} 0.8361123368260459

Parameter optimization finished!
OPTIMAL FOR ALL DATA:
XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=0.9, gamma=0.0, learning_rate=0.1,
       max_delta_step=0, max_depth=3, min_child_weight=7, missing=None,
       n_estimators=210, n_jobs=1, nthread=4, objective='binary:logistic',
       random_state=0, reg_alpha=25.0, reg_lambda=1,
       scale_pos_weight=8.285971685971687, seed=1, silent=True,
       subsample=0.8)
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
class weight scale : 8.285971685971687
              precision    recall  f1-score   support

           0       0.89      1.00      0.94     32191
           1       0.00      0.00      0.00      3885

   micro avg       0.89      0.89      0.89     36076
   macro avg       0.45      0.50      0.47     36076
weighted avg       0.80      0.89      0.84     36076

